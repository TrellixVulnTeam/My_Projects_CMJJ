{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongod start/running, process 968\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "service mongod status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'data_diggers', 'enron', 'local']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "client.database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading UK Universities ranking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the excel file\n",
    "import xlrd\n",
    "from collections import defaultdict\n",
    "uniRank_workbook=xlrd.open_workbook('../normalize_dataset/UK_Universities_ranking.xls', on_demand = True)\n",
    "uniRank_sheet=uniRank_workbook.sheet_by_index(0)\n",
    "\n",
    "# Create database and collection objects for UK Universities ranking\n",
    "db = client.data_diggers\n",
    "UniRankCollection = db.UniRank\n",
    "UniRankCollection.drop()\n",
    "ur=list()\n",
    "for i in range(1,uniRank_sheet.nrows-3):\n",
    "    row1 = uniRank_sheet.row_values(i)\n",
    "    dic1 = {'city_name':row1[0],'Number_of_universities':row1[1],'best_ranking':row1[2],'average_ranking':row1[3],'total_jobs_normalized_value':row1[4]}\n",
    "    ur.append(dic1)\n",
    "# print (ur)\n",
    "UniRankCollection.insert_many(ur)\n",
    "UniRankCollection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Total Jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the excel file\n",
    "import xlrd\n",
    "from collections import defaultdict\n",
    "totalJobs_workbook=xlrd.open_workbook('../normalize_dataset/total_jobs_new Z Score.xls', on_demand = True)\n",
    "totalJobs_sheet=totalJobs_workbook.sheet_by_index(0)\n",
    "\n",
    "# Create database and collection objects for Total jobs\n",
    "db = client.data_diggers\n",
    "totalJobsCollection = db.TotalJobs\n",
    "totalJobsCollection.drop()\n",
    "l=list()\n",
    "for i in range(1,totalJobs_sheet.nrows):\n",
    "    row = totalJobs_sheet.row_values(i)\n",
    "    dic2 = {'city_name':row[0],'total_jobs':row[2],'total_jobs_normalized_value':row[3]}\n",
    "    l.append(dic2)\n",
    "#print (l)\n",
    "totalJobsCollection.insert_many(l)\n",
    "totalJobsCollection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Road Traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the excel file\n",
    "import xlrd\n",
    "from collections import defaultdict\n",
    "roadTraffic_workbook=xlrd.open_workbook('../normalize_dataset/road traffic.xls', on_demand = True)\n",
    "roadTraffic_sheet=roadTraffic_workbook.sheet_by_index(0)\n",
    "# Create database and collection objects for road traffic and delay caused by it\n",
    "db = client.data_diggers\n",
    "roadTrafficCollection = db.roadTraffic\n",
    "db.drop_collection('roadTraffic')\n",
    "rt=list()\n",
    "for i in range(1,roadTraffic_sheet.nrows-3):\n",
    "    row2 = roadTraffic_sheet.row_values(i)\n",
    "    if row2[0]!='':\n",
    "        dic3 = {'city_name':row2[0].split(' UA')[0].split(' (Met County)')[0],'Delay 2015':row2[2],'Delay 2016':row2[3],'Delay_2015_normalized_value':row2[5],'Delay_2016_normalized_value':row2[6]}\n",
    "        rt.append(dic3)\n",
    "#print (l)\n",
    "roadTrafficCollection.insert_many(rt)\n",
    "roadTrafficCollection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of people exposed to road traffic noise 55dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the excel file\n",
    "import xlrd\n",
    "from collections import defaultdict\n",
    "trafficNoise_workbook=xlrd.open_workbook('../normalize_dataset/road_traffic_noise.xls', on_demand = True)\n",
    "trafficNoise_sheet=trafficNoise_workbook.sheet_by_index(0)\n",
    "# Create database and collection objects for Number of people exposed to road traffic noise 55 dB\n",
    "db = client.data_diggers\n",
    "trafficNoiseCollection = db.trafficNoise\n",
    "db.drop_collection('trafficNoise')\n",
    "tn=list()\n",
    "for i in range(1,trafficNoise_sheet.nrows):\n",
    "    row3 = trafficNoise_sheet.row_values(i)\n",
    "    dic4 = {'city_name':row3[0],'Number of people exposed to traffic noise':row3[1],'Normalized value of Number of people exposed to traffic noise':row3[2],}\n",
    "    tn.append(dic4)\n",
    "#print (l)\n",
    "trafficNoiseCollection.insert_many(tn)\n",
    "trafficNoiseCollection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happiness ranking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the excel file\n",
    "import xlrd\n",
    "from collections import defaultdict\n",
    "happiness_workbook=xlrd.open_workbook('../normalize_dataset/1.happiness_geo.xls', on_demand = True)\n",
    "happiness_sheet=happiness_workbook.sheet_by_index(1)\n",
    "# Create database and collection objects for Happiness rankings by Country\n",
    "db = client.data_diggers\n",
    "happinessCollection = db.happinessRank\n",
    "db.drop_collection('happinessRank')\n",
    "hl=list()\n",
    "for i in range(6,happiness_sheet.nrows):\n",
    "    row4 = happiness_sheet.row_values(i)\n",
    "    if row4[6]!= '':\n",
    "        if row4[1][0] == ' ':\n",
    "            row4[1] = row4[1][1:]\n",
    "        dic5 = {'city_name':row4[1],'Average happiness rating':row4[2],'Sample size':row4[6],'Normalized value of happiness ranking':row4[8]}\n",
    "        hl.append(dic5)\n",
    "happinessCollection.insert_many(hl)\n",
    "p = happinessCollection.find()\n",
    "# for i in p:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "l=list()\n",
    "filename ='../normalize_dataset/9.population.csv'\n",
    "with open(filename) as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    for row in f_csv:\n",
    "        rows=row[0].split('\\t')\n",
    "        l.append(rows)\n",
    "        \n",
    "db = client.data_diggers\n",
    "collection =db.population_collection\n",
    "population=list()\n",
    "collection.drop()\n",
    "for i in range(63):\n",
    "    population.append({\"city_name\":l[i+1][0],\"countries\":l[i+1][1],\"population\":l[i+1][2],\"N_population\":l[i+1][3]})\n",
    "collection.insert_many(population)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The House price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.drop_collection('houseprice_collection')\n",
    "collection =db.houseprice_collection\n",
    "collection.drop()\n",
    "import csv\n",
    "l=list()\n",
    "s =set()\n",
    "filename ='../normalize_dataset/7.UK-House price_Nor.csv'\n",
    "with open(filename) as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    for row in f_csv:\n",
    "        if row[0]!='Date':\n",
    "            l.append({\n",
    "                \"city_name\":row[1],\n",
    "                \"average_price\":row[3],\n",
    "                \"Normalization\":row[5]\n",
    "            })\n",
    "            s.add(row[1])\n",
    "    for city_name in s:\n",
    "        count = 0\n",
    "        summ = 0\n",
    "        nor_sum =0\n",
    "        for city in l:\n",
    "            if city['city_name'] == city_name:\n",
    "                summ += float(city['average_price'])\n",
    "                nor_sum += float(city['Normalization'])\n",
    "                count += 1\n",
    "        average_price = summ/float(count)\n",
    "        normalization = nor_sum/float(count)\n",
    "        collection.insert_one({\"city_name\":city_name,\"average_price\":average_price,\"Normalization\":normalization})\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gva_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.gva_collection\n",
    "collection.drop()\n",
    "\n",
    "import csv\n",
    "l=list()\n",
    "filename ='../normalize_dataset/10.gva_per_worker.csv'\n",
    "with open(filename) as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    for row in f_csv:\n",
    "        l.append({\n",
    "            \"city_name\":row[0],\n",
    "            \"GVA_per_worker_per_month\":row[1],\n",
    "            \"GVA_norm\":row[2],\n",
    "            \"GVA_max_min_norm\":row[3]    \n",
    "        })\n",
    "        collection.delete_one(l[0])\n",
    "    for i in range(1,63):\n",
    "        collection.insert_one(l[i])\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unemployment_max_min_norm': '0.3906976744',\n",
       " 'Unemployment_norm': '-0.4370771187',\n",
       " 'Unemployment_rate': '7.51',\n",
       " '_id': ObjectId('5a4d4cf77c4c7b78c0dc199e'),\n",
       " 'city_name': 'Aberdeen'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.unemployment\n",
    "collection.drop()\n",
    "import csv\n",
    "l=list()\n",
    "filename ='../normalize_dataset/11.unemployment_rate.csv'\n",
    "with open(filename) as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    for row in f_csv:\n",
    "        l.append({\n",
    "            \"city_name\":row[0],\n",
    "            \"Unemployment_rate\":row[1],\n",
    "            \"Unemployment_norm\":row[2],\n",
    "            \"Unemployment_max_min_norm\":row[3]    \n",
    "        })\n",
    "    collection.delete_one(l[0])\n",
    "    for i in range(1,63):\n",
    "        collection.insert_one(l[i])\n",
    "collection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database and collection objects for convenience\n",
    "db = client.data_diggers\n",
    "collection = db.reference\n",
    "import json\n",
    "with open('../cities.json', 'r') as f:\n",
    "    cities = json.load(f)\n",
    "collection.drop()\n",
    "collection.insert_many(cities)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1323"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.education\n",
    "import json\n",
    "with open('../normalize_dataset/education.json', 'r') as f:\n",
    "    education = json.load(f)\n",
    "\n",
    "l = []\n",
    "for edu in education:\n",
    "    l.append({'city_name':edu,'school_number_normalize_maxmin':education[edu]})\n",
    "\n",
    "collection.drop()\n",
    "collection.insert_many(l)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.hospitals\n",
    "import json\n",
    "with open('../normalize_dataset/hospitals.json', 'r') as f:\n",
    "    hospitals = json.load(f)\n",
    "\n",
    "l = []\n",
    "for hos in hospitals:\n",
    "    l.append({'city_name':hos,'hospitals_number_normalize_maxmin':hospitals[hos]})\n",
    "\n",
    "collection.drop()\n",
    "collection.insert_many(l)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5a4d4cf87c4c7b78c0dc27dd'),\n",
       " 'city_name': 'Mendip',\n",
       " 'pubs_number_normalize_maxmin': 171}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.pubs\n",
    "import json\n",
    "with open('../normalize_dataset/pubs.json', 'r') as f:\n",
    "    pubs = json.load(f)\n",
    "\n",
    "l = []\n",
    "for pub in pubs:\n",
    "    l.append({'city_name':pub,'pubs_number_normalize_maxmin':pubs[pub]})\n",
    "# l\n",
    "collection.drop()\n",
    "collection.insert_many(l)\n",
    "collection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rail_way station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.railway_station\n",
    "import json\n",
    "with open('../normalize_dataset/railway_station.json', 'r') as f:\n",
    "    stations = json.load(f)\n",
    "\n",
    "l = []\n",
    "for station in stations:\n",
    "    l.append({'city_name':station,'stations_number_normalize_maxmin':stations[station]})\n",
    "\n",
    "collection.drop()\n",
    "collection.insert_many(l)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the excel file\n",
    "import xlrd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "workbook_norm =xlrd.open_workbook('../normalize_dataset/WeatherNorm.xls', on_demand = True)\n",
    "sheet_norm=workbook_norm.sheet_by_index(0)    \n",
    "workbook =xlrd.open_workbook('../clean_dataset/15.Weather.xls', on_demand = True)\n",
    "sheet=workbook.sheet_by_index(0)    \n",
    "\n",
    "weather_by_station={}\n",
    "for i in range(1,sheet.nrows):\n",
    "    weather_station_inf = sheet.row_values(i)\n",
    "    weather_station_inf_norm = sheet_norm.row_values(i)\n",
    "    weather_by_station[weather_station_inf[0]] = {\"SummerDay_ave_temperature\":weather_station_inf[3],\n",
    "                                                  \"SummerDay_ave_temperature_norm\":weather_station_inf_norm[3],\n",
    "                                                  \"SummerNight_ave_temperature\":weather_station_inf[4],\n",
    "                                                  \"SummerNight_ave_temperature_norm\":weather_station_inf_norm[4],\n",
    "                                                  \"WinterDay_ave_temperature\":weather_station_inf[5],\n",
    "                                                  \"WinterDay_ave_temperature_norm\":weather_station_inf_norm[5],\n",
    "                                                  \"WinterNight_ave_temperature\":weather_station_inf[6],\n",
    "                                                  \"WinterNight_ave_temperature_norm\":weather_station_inf_norm[6],\n",
    "                                                  \n",
    "                                                  \"FrostDay_perYear\":weather_station_inf[7],\n",
    "                                                  \"FrostDay_perYear_norm\":weather_station_inf_norm[7],\n",
    "                                                  \"Rainfall_perMon\":weather_station_inf[8],\n",
    "                                                  \"Rainfall_perMon_norm\":weather_station_inf_norm[8],\n",
    "                                                  \"Sunshine_perMon\":weather_station_inf[9],\n",
    "                                                  \"Sunshine_perMon_norm\":weather_station_inf_norm[9]\n",
    "                                                 }\n",
    "# print(weather_by_station)\n",
    "import json\n",
    "with open('../normalize_dataset/weatherStationsToCities.json', 'r') as f:\n",
    "    stations = json.load(f)\n",
    "    \n",
    "l = []\n",
    "for station in stations:\n",
    "    l.append({\"city_name\":station,\n",
    "              \"SummerDay_ave_temperature\":weather_by_station[stations[station]][\"SummerDay_ave_temperature\"],\n",
    "              \"SummerDay_ave_temperature_norm\":weather_by_station[stations[station]][\"SummerDay_ave_temperature_norm\"],\n",
    "              \"SummerNight_ave_temperature\":weather_by_station[stations[station]][\"SummerNight_ave_temperature\"],\n",
    "              \"SummerNight_ave_temperature_norm\":weather_by_station[stations[station]][\"SummerNight_ave_temperature_norm\"],\n",
    "              \"WinterDay_ave_temperature\":weather_by_station[stations[station]][\"WinterDay_ave_temperature\"],\n",
    "              \"WinterDay_ave_temperature_norm\":weather_by_station[stations[station]][\"WinterDay_ave_temperature_norm\"],\n",
    "              \"WinterNight_ave_temperature\":weather_by_station[stations[station]][\"WinterNight_ave_temperature\"],\n",
    "              \"WinterNight_ave_temperature_norm\":weather_by_station[stations[station]][\"WinterNight_ave_temperature_norm\"],\n",
    "              \"FrostDay_perYear\":weather_by_station[stations[station]][\"FrostDay_perYear\"],\n",
    "              \"FrostDay_perYear_norm\":weather_by_station[stations[station]][\"FrostDay_perYear_norm\"],\n",
    "              \"Rainfall_perMon\":weather_by_station[stations[station]][\"Rainfall_perMon\"],\n",
    "              \"Rainfall_perMon_norm\":weather_by_station[stations[station]][\"Rainfall_perMon_norm\"],\n",
    "              \"Sunshine_perMon\":weather_by_station[stations[station]][\"Sunshine_perMon\"],\n",
    "              \"Sunshine_perMon_norm\":weather_by_station[stations[station]][\"Sunshine_perMon_norm\"]\n",
    "             })\n",
    "\n",
    "db = client.data_diggers\n",
    "collection = db.weather\n",
    "# print(l)\n",
    "collection.drop()\n",
    "collection.insert_many(l)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check who database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather',\n",
       " 'houseprice_collection',\n",
       " 'education',\n",
       " 'training_data',\n",
       " 'unemployment',\n",
       " 'trafficNoise',\n",
       " 'hospitals',\n",
       " 'population_collection',\n",
       " 'gva_collection',\n",
       " 'UniRank',\n",
       " 'happinessRank',\n",
       " 'railway_station',\n",
       " 'pubs',\n",
       " 'roadTraffic',\n",
       " 'reference',\n",
       " 'TotalJobs']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = client.data_diggers\n",
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
